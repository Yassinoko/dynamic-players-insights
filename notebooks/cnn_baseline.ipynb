{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cb6ca0-6331-4869-a7c4-ba7d9a4aae30",
   "metadata": {},
   "source": [
    "## Computing baseline of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e0ce6-8b06-4e8e-a020-754c1f00566d",
   "metadata": {},
   "source": [
    "#### Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c98909-2dad-41c1-a03c-90edfed77702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from tensorflow import stack\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import Resizing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a823cef3-4c0b-4ece-9197-3a3cf7dde412",
   "metadata": {},
   "source": [
    "#### Upload images and organize them in the Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddda61d8-6934-4605-adab-140f130a9f19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of all files and folders in the specified directory\n",
    "directory_path = \"raw_data/faces\"\n",
    "img_folders = os.listdir(directory_path)\n",
    "labels = [' '.join(folder.replace('Man.', '').replace('Real', '').split()[:-1]) for folder in img_folders]\n",
    "len(img_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1059f86c-f5ea-437c-a4c9-3fb9bceb1f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_label_dict = {'image': [], 'name': []}\n",
    "shapes = []\n",
    "\n",
    "for img_folder, label in zip(img_folders, labels):\n",
    "    directory_path = f\"raw_data/faces/{img_folder}\"\n",
    "    img_files = os.listdir(directory_path)\n",
    "\n",
    "    for img_file in img_files:\n",
    "        try:\n",
    "            image_path = f\"raw_data/faces/{img_folder}/{img_file}\"\n",
    "            # image_tf = load_img(image_path)\n",
    "            # image_np = img_to_array(image_tf)\n",
    "\n",
    "            image_pil = Image.open(image_path)\n",
    "            image_np = np.array(image_pil)\n",
    "\n",
    "            shapes.append(image_np.shape)\n",
    "\n",
    "            img_label_dict['image'].append(image_np)\n",
    "            img_label_dict['name'].append(label)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa60f2d-197e-4f22-9331-253684892021",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(408, 612, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_common(lst):\n",
    "    counter = Counter(lst)\n",
    "    most_common_item = max(counter, key=counter.get)\n",
    "    return most_common_item\n",
    "\n",
    "common = most_common(shapes)\n",
    "common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733d8fd8-7b46-40d9-84a3-f53749999bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 408, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_label_dict['image'][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "761c9413-fb4a-4afd-ad9a-4c2caa7fb3c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 408, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(img_label_dict['image'][0]).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db6729-6cf9-4034-a086-2fdf0cea8762",
   "metadata": {},
   "source": [
    "#### Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f48585-e014-4e41-8481-594ad72faf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=common))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44339dcb-1f04-4eb8-b397-351586465166",
   "metadata": {},
   "source": [
    "#### Preprocess the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52d50930-3949-4aa4-872c-3911be1b9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = common[0]\n",
    "width = common[1]\n",
    "\n",
    "resize = layers.Resizing(height, width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bcd378-e3c6-433a-ab07-a2cb80aaacad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_img = []\n",
    "\n",
    "for img in img_label_dict['image']:\n",
    "    try:\n",
    "        preprocessed_img.append(resize(img))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2efb890-587f-44e4-9ced-3523f8e5e1af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3900, 408, 612, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_img = np.array(preprocessed_img)\n",
    "preprocessed_img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736b4cb8-50f3-4803-ad38-88a40df0c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3301\n",
      "3589\n"
     ]
    }
   ],
   "source": [
    "print(shapes.index((408, 612)))\n",
    "print(shapes.index((418, 612)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aea0a5f-81d1-4b76-8d14-987c215e1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = img_label_dict['name']\n",
    "del labels[shapes.index((408, 612))]\n",
    "del labels[shapes.index((418, 612))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8a2a4-ae69-41b1-9dfc-c4b05a168b03",
   "metadata": {},
   "source": [
    "#### Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cc89d2a-f576-43f0-991c-9f9531188ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_img[:100]\n",
    "y = np.array(labels[:100])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c505a16a-c6db-470d-91df-978734e5506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0eb5be-cc89-4b19-8dd5-2392aa556a6a",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14f469c1-5774-4b80-9257-eb3b80415224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 408, 612, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54398fa8-0b35-4dcb-8859-b791b18bf8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.0000e+00 - accuracy: 0.0600\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 6s 2s/step - loss: 0.0000e+00 - accuracy: 0.0600\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.0000e+00 - accuracy: 0.0600\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.0000e+00 - accuracy: 0.0600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x142f4a0e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='accuracy', patience=3)\n",
    "\n",
    "model = initialize_model()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d8f96-347f-461e-ae2c-f9f669a2cf8a",
   "metadata": {},
   "source": [
    "#### Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ae60ec5-a13b-4f9e-91d9-031be1855f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 342ms/step - loss: 0.0000e+00 - accuracy: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.019999999552965164]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f7db3-c79b-4f95-ac86-387362bc4e88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
